### <font color="#00000">The Second Edition</font>

The years 2015 and 2016 saw seismic changes in Spark, culminating in the release of Spark 2.0 in July of 2016. The most salient of these changes are the modifications to Spark’s core API. In versions prior to Spark 2.0, Spark’s API centered around Resilient Distributed Datasets (RDDs), which are lazily instantiated collections of objects, partitioned across a cluster of computers.

2015年和2016年，Spark发生了地震般的变化，最终于2016年7月发布了Spark 2.0。这些变化中最突出的是Spark的核心API的修改。在Spark 2.0之前的版本中，Spark的API以弹性分布式数据集（RDD）为中心，这是一个惰性的实例化对象集合，分布在一组计算机上。


Although RDDs enabled a powerful and expressive API, they suffered two main problems. First, they didn’t lend themselves well to performant, stable execution. By relying on Java and Python objects, RDDs used memory inefficiently and exposed Spark programs to long garbage-collection pauses. They also tied the execution plan into the API, which put a heavy burden on the user to optimize the execution of their program. For example, where a traditional RDBMS might be able to pick the best join strategy based on the size of the tables being joined, Spark required users to make this choice on their own. Second, Spark’s API ignored the fact that data often fits into a structured relational form, and when it does, an API can supply primitives that makes the data much easier to manipulate, such as by allowing users to refer to column names instead of ordinal positions in a tuple.

<font style="background:yellow;">虽然RDD启用了功能强大且富有表现力的API，但它们遇到了两个主要问题。首先，他们并没有很好地适应高性能，稳定的执行。通过依赖Java和Python对象，RDD使用内存效率低下并将Spark程序暴露给长时间的垃圾收集暂停。他们还将执行计划绑定到API，这给用户带来了沉重的负担，以优化他们的程序执行。例如，传统的RDBMS可能能够根据要连接的表的大小选择最佳的连接策略，Spark要求用户自己做出这个选择。其次，Spark的API忽略了这样一个事实：数据通常融入结构化的关系形式，并且当它发生时，API可以提供使数据更容易操作的基本组件，例如允许用户引用列名而不是在一个元组的序数位置</font>。

Spark 2.0 addressed these problems by replacing RDDs with Datasets and DataFrames. Datasets are similar to RDDs but map the objects they represent to encoders, which enable a much more efficient in-memory representation. This means that Spark programs execute faster, use less memory, and run more predictably. Spark also <font color="#32cd32"><strong>places </strong></font> an optimizer <font color="#32cd32"><strong>between</strong></font> data sets <font color="#32cd32"><strong>and</strong></font> their execution plan, which means that it can make more intelligent decisions about how to execute them. DataFrame is a subclass of Dataset that is specialized to model relational data (i.e., data with rows and fixed sets of columns). By understanding the notion of a column, Spark can offer a cleaner, expressive API, as well as enable a number of performance optimizations. For example, if Spark knows that only a subset of the columns are needed to produce a result, it can avoid materializing those columns into memory. And many transformations that previously needed to be expressed as user-defined functions (UDFs) are now expressible directly in the API. This is especially advantageous when using Python, because Spark’s internal machinery can execute transformations much faster than functions defined in Python. DataFrames also offer interoperability with Spark SQL, meaning that users can write a SQL query that returns a data frame and then use that DataFrame programmatically in the Spark-supported language of their choice. Although the new API looks very similar to the old API, enough details have changed that nearly all Spark programs need to be updated.

<font style="background:yellow;">Spark 2.0通过用 Dataset 和 DataFrame 替换RDD解决了这些问题。Dataset 与 RDD 类似，但将它们所代表的对象映射到编码器，从而实现更高效的内存中表示。这意味着Spark程序执行速度更快，占用内存更少，运行更可预测。 Spark 还在数据集合及其执行计划之间放置了一个优化器，这意味着它可以就如何执行它们做出更明智的决策。 DataFrame是Dataset的子类，专门用于建模关系数据（即具有行和固定列集的数据）。通过理解列的概念，Spark可以提供更清晰，更具表现力的API，并支持许多性能优化。例如，如果Spark知道只需要生成结果的列的子集，则可以避免将这些列实现到内存中。以前需要表达为用户定义函数（UDF）的许多转换现在可以直接在API中表达。这在使用Python时尤其有利，因为Spark的内部机制可以比Python中定义的函数更快地执行转换。 DataFrame 还提供与Spark SQL的互操作性，这意味着用户可以编写返回 DataFrame 的SQL查询，然后以他们选择的Spark支持的语言以编程方式使用该DataFrame。虽然新的API看起来与旧的API非常相似，但是有足够的细节已经改变，几乎所有的Spark程序都需要更新</font>。

In addition to the code API changes, Spark 2.0 saw big changes to the APIs used for machine learning and statistical analysis. In prior versions, each machine learning algorithm had its own API. Users who wanted to prepare data for input into algorithms or to feed the output of one algorithm into another needed to write their own custom <font color="#32cd32"><strong>orchestration </strong></font> code. Spark 2.0 contains the Spark ML API, which introduces a framework for composing pipelines of machine learning algorithms and feature transformation steps. The API, inspired by Python’s popular Scikit-Learn API, revolves around estimators and transformers, objects that learn parameters from the data and then use those parameters to transform data. The Spark ML API is heavily integrated with the DataFrames API, which makes it easy to train machine learning models on relational data. For example, users can refer to features by name instead of by ordinal position in a feature vector.

除了代码API更改之外，Spark 2.0 还对用于机器学习和统计分析的API进行了重大更改。在以前的版本中，每个机器学习算法都有自己的API。想要为输入算法准备数据或将一种算法的输出提供给另一种算法的用户需要编写自己的自定义编排代码。 Spark 2.0 包含 Spark ML API，它引入了一个框架，用于组成机器学习算法和特征转换步骤的管道。该 API 受 Python 流行的 Scikit-Learn API 的启发，围绕估算器（estimator）和转换器（transformer），从数据中学习参数然后使用这些参数转换数据的对象。 Spark ML API与DataFrames API高度集成，可以轻松地在关系数据上训练机器学习模型。例如，用户可以通过名称而不是特征向量中的顺序位置来引用特征。

Taken together, all these changes to Spark have <font color="#32cd32"><strong>rendered</strong></font> much of the first edition <font color="#32cd32"><strong>obsolete</strong></font>. This second edition updates all of the chapters to use the new Spark APIs when possible. Additionally, we’ve cut some bits that are no longer relevant. For example, we’ve removed a full appendix that dealt with some of the intricacies of the API, in part because Spark now handles these situations intelligently without user intervention. With Spark in a new era of maturity and stability, we hope that these changes will preserve the book as an useful resource on analytics with Spark for years to come. 

综合起来，Spark的所有这些变化都使得第一版的大部分内容都过时了。第二版更新了所有章节，以便尽可能使用新的Spark API。另外，我们削减了一些不再相关的一小部分。例如，我们删除了一个处理API复杂性的完整附录，部分原因是Spark现在可以智能地处理这些情况而无需用户干预。随着Spark进入成熟和稳定的新时代，我们希望这些变化能够在未来几年内将本书作为Spark分析的有用资源。



