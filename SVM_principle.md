---
title: 统计学习方法第7章-支持向量机
mathjax: true
mathjax2: true
categories: 中文
tags: [Machine Learning, 统计学习方法]
date: 2018-10-01
comments: true
copyright: true
toc: true
top: true
---

**注**：李航这一章节除了核空间和SMO(序列最小优化)算法，其他讲的都比较容易理解，文中在恰当地方将会给出更好的参考资料。

支持向量机(support vector machines, SVM)是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器；支持向量机还包括核技巧，这使它成为实质上的非线性分类器。支持向量机的学习策略就是**间隔最大化**，可形式化为一个求解凸二次规划(convex quadratic programming)的问题，也等价于正则化的合页损失函数的最小化问。支持向量机的学习算法是**求解凸二次规划的最优化算法。**

**支持向量机学习模型：**线性可分支持向量机(linear support vector machine in linearly separable case )、线性支持向量机(linear support vector machine)及非线性支持向量机(non-linear support vector machine)。**学习方法包括：**硬间隔最大化(hard margin maximization)、软间隔最大化(soft margin maximization)、核技巧(kernel trick)。通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机。这样的方法称为核技巧(kernel trick)

## 7.1 线性可分支持向量机与硬间隔最大化

**定义7.1 (线性可分支持向量机)：**给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为s

![](https://images0.cnblogs.com/blog/790160/201508/281740494379215.png)

以及相应的分类决策函数

![](https://images0.cnblogs.com/blog/790160/201508/281740500159559.png)

称为线性可分支持向量机.

**函数间隔和几何间隔**

**定义7.2(函数间隔  functiona lmargin****)：**对于给定的训练数据集T和超平面(w, b)，定义超平面关于样本点(xi, yi)的函数间隔为

![0fdead85-e023-4225-8a7e-beca9b97a554](https://images0.cnblogs.com/blog/790160/201508/281740507509715.png)

定义超平面(w,b)关于训练数据集T的函数间隔为超平面(w,b)关于T中所有样本点(xi, yi)的函数间隔之最小值，即

![](https://images0.cnblogs.com/blog/790160/201508/281740513442532.png)

函数间隔可以表示分类预测的正确性及确信度。但是成比例地改变w和b，例如将它们改为2w和2b，超平面并没有改变，但函数间隔却成为原来的2倍。

对分离超平面的法向量、加某些约束，如规范化，||w||=1，使得间隔是确定的。这时函数间隔成为几何间隔。

**定义7.2(几何间隔 geometric margin)：**对于给定的训练数据集T和超平面(w, b)，定义超平面关于样本点(xi, yi)的函数间隔为

![](https://images0.cnblogs.com/blog/790160/201508/281740519535417.png)

定义超平面(w,b)关于训练数据集T的函数间隔为超平面(w,b)关于T中所有样本点(xi, yi)的函数间隔之最小值，即

![](https://images0.cnblogs.com/blog/790160/201508/281740530787860.png)

函数间隔和几何间隔的关系:

![](https://images0.cnblogs.com/blog/790160/201508/281740541874534.png)

如果超平面参数w和b成比例地改变(超平面没有改变)，函数间隔也按此比例改变，而几何间隔不变。

**间隔最大化**

支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。对线性可分的训练数据集而言，线性可分分离超平面有无穷多个(等价于感知机)，但是几何间隔最大的分离超平面是唯一的。这里的间隔最大化又称为硬间隔最大化。   

间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类，也就是说，不仅将正负实例点分开，而且对最难分的实例点(离超平面最近的点)也有足够大的确信度将它们分开。

线性可分支持向量机的学习算法——最大间隔法(maximum margin method)。

![](https://images0.cnblogs.com/blog/790160/201508/281740554374249.png)

**定理7.1 (最大间隔分离超平面的存在唯一性)：**若训练数据集z线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。

**支持向量和间隔边界**

在线性可分情况下，训练数据集的样本点中与分离超平面跄离最近的样本点的实例称为**支持向量( support vector )。**支持向量是使约束条件式等号成立的点，即

![](https://images0.cnblogs.com/blog/790160/201508/281740563597163.png)

对yi=+1的正例点，支持向量在超平面![](https://images0.cnblogs.com/blog/790160/201508/281740574067350.png)

对yi=-1的负例点，支持向量在超平面![](https://images0.cnblogs.com/blog/790160/201508/281740590782894.png)

下图中在H1和H2上的点就是支持向量

![](https://images0.cnblogs.com/blog/790160/201508/281741030003325.png)

H1和H2之间的距离称为间隔(margin)。间隔依赖于分离超平面的法向量w，等于2/||w||。H1和H2称为间隔边界.

在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。如果移动支持向量将改变所求的解；但是如果移动其他实例点，甚至去掉这些点，则解是不会改变的。由于支持向量在确定分离超平面中起决定性作用，所以将这种分类模型称为支持向量机。支持向量的个数一般很少，所以支持向量机由很少的“重要的”训练样本确定。

**学习的对偶算法****(dual algorithn)**

构建拉格朗日函数(Lagrange function)，引进拉格朗日乘子(Lagrange multiplier)：

![](https://images0.cnblogs.com/blog/790160/201508/281741037192413.png)

根据拉格朗日对偶性，原始问题的对偶问题是拉格朗日函数的极大极小问题

![](https://images0.cnblogs.com/blog/790160/201508/281741042811284.png)

**定理7.2：**设a*是对偶最优化问题的解，则存在下标j使得aj* >0，并可按下式求得原始最优化问题的解:

![](https://images0.cnblogs.com/blog/790160/201508/281741063903129.png)

**算法7.2(线性可分支持向量机学习算法)**

![](https://images0.cnblogs.com/blog/790160/201508/281741071878772.png)

**定义7.4(支持向量)**：将训练数据集中对应于ai*>0的样本点(xi, yi)的实例xi称为支持向量。支持向量**一定在间隔边界上。**

![](https://images0.cnblogs.com/blog/790160/201508/281741079371402.png)

## 7.2 线性支持向量机与软间隔最大化

针对线性不可分训练数据：

线性不可分意味着某些样本点不能满足函数间隔大于等于1的约束条件,为了解决这个问题，可以对每个样本点引进一个松弛变量，约束条件变为：

![](https://images0.cnblogs.com/blog/790160/201508/281741096569959.png)

线性不可分的线性支持向量机的学习问题变成凸二次规划(convex quadratic progamming )问题（原始问题）:

![](https://images0.cnblogs.com/blog/790160/201508/281741105783874.png)

**定义7.5(线性支持向量机)：**对于给定的线性不可分的训练数据集，通过求解凸二次规划问题，即软间隔最大化问题。得到的分离超平面为

![](https://images0.cnblogs.com/blog/790160/201508/281741112346475.png)

以及相应的分类决策函数

![](https://images0.cnblogs.com/blog/790160/201508/281741120004875.png)

称为线性支持向量机.

**学习的对偶算法****(dual algorithn) ：拉格朗日函数**

通过求解对偶问题得到

**定理7.3：**设a*是对偶最优化问题的一个解，则存在一个分量0<aj* <C，可得原始最优化问题的解:

![](https://images0.cnblogs.com/blog/790160/201508/281741128441234.png)

**算法7.3(线性支持向量机学习算法)**

![](https://images0.cnblogs.com/blog/790160/201508/281741138446406.png)

**支持向量**

线性不可分的情况下，将对偶问题的解a*中对应于aj* > 0的样本点(xi，yi)的实例xi称为支持向量(软间隔的支持向量)。

![](https://images0.cnblogs.com/blog/790160/201508/281741145312549.png)

软间隔的支持向量xi或者在间隔边界上，或者在间隔边界与分离超平面之间，或者在分离超平面误分一侧。

若a*<C，则约束 ![](https://images0.cnblogs.com/blog/790160/201508/281741152349163.png)，支持向量xi恰好落在间隔边界上；

若a*<C，0<约束<1，则分类正确，xi在间隔边界与分离超平面之间;

若a*<C，约束=1，则xi在分离超平面上:

若a*<C，约束>1，则xi位于分离超平面误分一侧.

**合页损失函数**

线性支持向量机学习还有另外一种解释，就是最小化以下目标函数

![](https://images0.cnblogs.com/blog/790160/201508/281741160007563.png)

目标函数的第1项是经验损失或经验风险，函数

![](https://images0.cnblogs.com/blog/790160/201508/281741164693108.png)

称为合页损失函数(hinge loss ftmction)

**定理7.4** 线性支持向量机原始最优化问题等价于最优化问题：

![](https://images0.cnblogs.com/blog/790160/201508/281741170947465.png)

![](https://images0.cnblogs.com/blog/790160/201508/281741178449094.png)

合页损失函数不仅要分类正确，而且确信度足够高时损失才是0。

## 7.3 非线性支持向量机与核函数

**核技巧** 
非线性分类问题：如果能用Rn中的一个超曲面将正负例正确分开，则称这个问题为非线性可分问题.

求解方法：进行非线性变换，将非线性问题变成线性问题。

核技巧应用到支持向量机，其基本想法就是通过一个非线性变换将输入空间(欧氏空间Rn或离散集合)对应于一个特征空间(希尔伯特空间H)，使得在输入空间Rn中的超曲面模型对应于特征空间H中的超平面模型(支持向量机)。

**定义7.6（核函数）**设X是输入空间，H为特征空间，如果存在一个映射映射函数

![](https://images0.cnblogs.com/blog/790160/201508/281741183902195.png)

使得对所有属于X的x,z，函数K(x,z)满足条件

![](https://images0.cnblogs.com/blog/790160/201508/281741191569594.png)

**则称K(x,z)为核函数。**

核技巧的想法是，在学习与预测中只定义核函数K(x,z)，而不显式地定义映射函数。对于给定的核K(x,z)，特征空间x和映射函数的取法并不唯一，可以取不同的特征空间，即便是在同一特征空间里也可以取不同的映射。

核技巧在支持向量机中的应用

在对偶问题的目标函数中的内积(xi*xj)可以用核函数K(xi, xj)来代替：

![](https://images0.cnblogs.com/blog/790160/201508/281741198597210.png)

分类决策函数也可用核函数代替，变为：

![](https://images0.cnblogs.com/blog/790160/201508/281741205943068.png)

这等价于经过映射函数将原来的输入空间变换到一个新的特征空间，将输入空间中的内积(xi*xj)变换为特征空间中的内积 ![](https://images0.cnblogs.com/blog/790160/201508/281741210788681.png)。在新的特征空间里从训练样本中学习线性支持向量机。当映射函数是非线性函数时，学习到的含有核函数的支持向量机是非线性分类模型。

在核函数给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。学习是隐式地在特征空间进行的，不需要显式地定义特征空间和映射函数。这样的技巧称为核技巧。

**正定核(**positive definite kernel function)的充要条件

**定理7.5(正定核的充要条件)** 设K:X*X->R是对称函数，则K(x,z)为正定核函数的充要条件是对任意xi属于X，i=1,2,... ,m,  K(x,z)对应的Gram矩阵

![](https://images0.cnblogs.com/blog/790160/201508/281741221567111.png)

是半正定矩阵.

**定义7.7 (正定核的等价定义)**

设X包含于Rn，K(x,z)为定义在X*X上的对称函数，如果对任意xi属于X，i=1,2,... ,m,  K(x,z)对应的Gram矩阵

![](https://images0.cnblogs.com/blog/790160/201508/281741229068740.png)

是半正定矩阵，则称K(x,z)是正定核。

**常用核函数**

（1） 多项式核函数（polynomial kernel function）

![](https://images0.cnblogs.com/blog/790160/201508/281741237343627.png)

对应的支持向量机是一个P次多项式分类器.在此情形下，分类决策函数成为

![](https://images0.cnblogs.com/blog/790160/201508/281741247657042.png)

（2）高斯核函数（Gaussian kernel  function）

![](https://images0.cnblogs.com/blog/790160/201508/281741257344971.png)

对应的支持向量机是一个高斯径向基函数( radial basis function )分类器.在此情形下，分类决策函数成为

![](https://images0.cnblogs.com/blog/790160/201508/281741265154844.png)

（3）字符串核函数(string kernel function)

    核函数不仅可以定义在欧氏空间上，还可以定义在离散数据的集合上。比如，字符串核是定义在字符串集合上的核函数。

两个字符串s和t上的字符串核函数是基于映射

![](https://images0.cnblogs.com/blog/790160/201508/281741271409201.png)

的特征空间中的内积

![](https://images0.cnblogs.com/blog/790160/201508/281741277503087.png)

字符串核函数气kn(s, t)给出了字符串s和t中长度等于n的所有子串组成的特征向量的余弦相似度(cosine similuity)。直观上，两个字符串相同的子串越多，它们就越相似，字符串核函数的值就越大。字符串核函数可以由动态规划快速地计算。

**非线性支持向量分类机**

**定义7.8 (非线性支持向量机)** 从非线性分类训练集，通过核函数与软间隔最大化，或凸二次规划，学习得到的分类决策函数

![](https://images0.cnblogs.com/blog/790160/201508/281741284065687.png)

称为非线性支持向量，K(x,z)是正定核函数

![](https://images0.cnblogs.com/blog/790160/201508/281741294535873.png)

## 7.4 序列最小最优化算法(sequential minimal opfimization，SMO)

SMO算法要解如下凸二次规划的对偶问题：

![](https://images0.cnblogs.com/blog/790160/201508/281741302653989.png)

SMO算法是一种启发式算法，其基本思路是：如果所有变量的解都满足此最优化问题的KKT条件(Karush-Kuhn-Tucker conditions)，那么这个最优化问题的解就得到了。因为KKT条件是该最优化问题的充分必要条件。否则，选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题。这个二次规划问题关于这两个变量的解应该更接近原始二次规划问题的解，因为这会使得原始二次规划问题的目标函数值变得更小。重要的是，这时子问题可以通过解析方法求解，这样就可以大大提高整个算法的计算速度。子问题有两个变量，一个是违反KKT条件最严重的那一个，另一个由约束条件自动确定。如此，SMO算法将原问题不断分解为子问题并对子问题求解，进而达到求解原问题的目的。

整个SMO算法包括两个部分:求解两个变量二次规划的解析方法和选择变量的启发式方法.

**两个变量二次规划的求解方法**

以下如有不懂请参考：[机器学习算法实践-SVM中的SMO算法 - PytLab酱的文章 - 知乎](https://zhuanlan.zhihu.com/p/29212107)

不失一般性，假设选择的两个变量是a1，a2，其他变量ai (i=3,4, ..., N)是固定的。SMO的最优化问题的子问题可以写成:

![](https://images0.cnblogs.com/blog/790160/201508/281741310312390.png)

假设初始可行解为a1old和a2old，最优解为a1new和a2new，满足

![](https://images0.cnblogs.com/blog/790160/201508/281741317657247.png)

L与H是a2new所在的对角线段端点的界

![](https://images0.cnblogs.com/blog/790160/201508/281741323594362.png)

如果y1 != y2,

![](https://images0.cnblogs.com/blog/790160/201508/281741330621977.png)

如果y1 = y2,

![](https://images0.cnblogs.com/blog/790160/201508/281741338441850.png)

根据约束条件，有

![](https://images0.cnblogs.com/blog/790160/201508/281741346099249.png)

引入记号，

![](https://images0.cnblogs.com/blog/790160/201508/281741356569435.png)

![](https://images0.cnblogs.com/blog/790160/201508/281741364844322.png)

![](https://images0.cnblogs.com/blog/790160/201508/281741376723253.png)

代入到最优化问题中，有

![](https://images0.cnblogs.com/blog/790160/201508/281741386875195.png)

这样，通过对W进行求导可以得到

**定理7.6** 最优化问题沿着约束方向未经剪辑时的解是

![](https://images0.cnblogs.com/blog/790160/201508/281741396871368.png) 经剪辑后的解是

![](https://images0.cnblogs.com/blog/790160/201508/281741413904155.png)

**变量的选择方法**

第1个变量的选择

    SMO称选择第1个变量的过程为外层循环。外层循环在训练样本中选取违

反KKT条件最严重的样本点，并将其对应的变量作为第1个变量。具体地，检

验训练样本点(xi,yi)是否满足KKT条件，即

![](https://images0.cnblogs.com/blog/790160/201508/281741432814998.png)

该检验是在 ![](http://images0.cnblogs.com/blog/790160/201508/281741443128414.png)的样本点，即在间隔边界上的支持向量点，检验它们是否满足KKT条件。如果这些样本点都满足KKT条件，那么遍历整个训练集，检验它们是否满足KKT条件。

**第2个变量的选择**

SMO称选择第2个变量的过程为内层循环。假设在外层循环中已经找到第1个变量a1，现在要在内层循环中找第2个变量a2。第2个变量选择的标准是希望使a2有足够大的变化.

由定理7.6可知a2是依赖于|E1-E2|的，为了加快计算速度，一种简单的做法是选择a2使其对应的|E1-E2|最大。因为a1已定，E1也确定了。如果E1是正的，那么选择最小的Ei作为E2；如果E1是负的，那么选择最大的Ei作为E2。在特殊情况下，如果内层循环通过以上方法选择的a2不能使目标函数有足够的下降，那么采用以下启发式规则继续选择a2。遍历在间隔边界上的支持向量点，依次将其对应的变量作为a2试用，直到目标函数有足够的下降。若找不到合适的a2，那么遍历训练数据集；若仍找不到合适的a2，则放弃第1个a1，再通过外层循环寻求另外的a1。

**计算阈值b和差值Ei**

由变量选择的检验条件可得，

如果 ![](https://images0.cnblogs.com/blog/790160/201508/281741458449915.png)，

![](https://images0.cnblogs.com/blog/790160/201508/281741463286529.png)

如果 ![](https://images0.cnblogs.com/blog/790160/201508/281741472035430.png)，

![](https://images0.cnblogs.com/blog/790160/201508/281741482031602.png)

    如果a1new和a2new同时满足条件 ![](https://images0.cnblogs.com/blog/790160/201508/281741488594203.png)。如果a1new和a2new是0或者C，那么b1new和b2new以及它们之间的数都是符合KKT条件的阈值，这时选择它们的中点作为bnew。

Ei值的更新要用到bnew值，以及所有支持向量对应的aj

![](https://images0.cnblogs.com/blog/790160/201508/281741506727490.png)

其中S是所有支持向量xj的集合

![](https://images0.cnblogs.com/blog/790160/201508/281741517501219.png)

参考：

1. 李航《统计学习方法》